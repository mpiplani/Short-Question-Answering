{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Used to generate the same set of random numbers when random function is called\n",
    "np.random.seed(7)\n",
    "\n",
    "#Read The TSV Data File\n",
    "data = pd.read_table('Datasets/train.tsv')\n",
    "data1 = pd.read_table('Datasets/train_rel_2.tsv')\n",
    "\n",
    "#Because of small size of the dataset O merged both the files given for training\n",
    "data = [data, data1]\n",
    "data = pd.concat(data)\n",
    "\n",
    "\n",
    "essay_text = data['EssayText']\n",
    "essay_score = data['Score1']\n",
    "essay_set = data['EssaySet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34250 texts.\n",
      "Found 16729 unique tokens.\n",
      "Shape of data tensor: (34250, 500)\n",
      "Shape of label tensor: (34250, 4)\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "# RAW DATA ENCODING\n",
    "# Format the text samples and labels into tensors that can be fed into a neural network\n",
    "##\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "sets = [0,1,2,3] # The 4 different classes of Scores\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for label in sets:\n",
    "\tlabel_id = len(labels_index)\n",
    "\tlabels_index[label] = label_id\n",
    "\tfor t in essay_text[data['Score1']==label]:\n",
    "\t\ttexts.append(t)\n",
    "\t\tlabels.append(label_id)\n",
    "\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "top_words = 5000 #top most-frequent words extracted from the dataset\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=top_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_response_length = 500\n",
    "data = pad_sequences(sequences, maxlen=max_response_length)\n",
    "\n",
    "#Convert class vector to binary class matrix, for use with categorical_crossentropy.\n",
    "#Or in simple words to convert numbers into ONE-HOT Vector for MultiClass classification\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "#Shuffle the dataset \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Set of stopwords from nltk.corpus package\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "word_counts_X = []  # List of word Counts in each example response\n",
    "for x in essay_text:\n",
    "    word_counts_X.append(len([i for i in word_tokenize(x) if i not in stop]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_counts = []  # Set of sentence counts in each example response\n",
    "for x in essay_text:\n",
    "    sent_counts.append(len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "avg_word_length= []\n",
    "for x in essay_text:\n",
    "    clean_essay = re.sub(r'\\W', ' ', x)\n",
    "    words = word_tokenize(clean_essay)\n",
    "    avg_word_length.append(sum(len(word) for word in words) / len(words))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence into words\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lemmas = []\n",
    "for x in essay_text:\n",
    "    tokenized_sentences = tokenize(x)      \n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "        \n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    count_lemmas.append(lemma_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_spell_error = []\n",
    "data_file = open('Datasets/big.txt').read()\n",
    "words_ = re.findall('[a-z]+', data_file.lower())\n",
    "word_dict = collections.defaultdict(lambda: 0)\n",
    "for word in words_:\n",
    "    word_dict[word] += 1\n",
    "for x in essay_text:\n",
    "\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(x).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "    \n",
    "                        \n",
    "    mispell_count = 0\n",
    "    \n",
    "    words = clean_essay.split()\n",
    "                        \n",
    "    for word in words:\n",
    "        if not word in word_dict:\n",
    "            mispell_count += 1\n",
    "    \n",
    "    count_spell_error.append(mispell_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_noun=[]\n",
    "count_adj=[]\n",
    "count_verb =[]\n",
    "count_adv=[]\n",
    "for x in essay_text:\n",
    "    tokenized_sentences = tokenize(x)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    count_noun.append(noun_count)\n",
    "    count_adj.append(adj_count)\n",
    "    count_verb.append(verb_count)\n",
    "    count_adv.append(adv_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle all of the data\n",
    "import numpy as np\n",
    "\n",
    "word_counts_X = np.array(word_counts_X)\n",
    "np.save(\"word_counts_X\",word_counts_X)\n",
    "\n",
    "\n",
    "sent_counts = np.array(sent_counts)\n",
    "np.save(\"sent_counts\",sent_counts)\n",
    "\n",
    "\n",
    "avg_word_len = np.array(avg_word_length)\n",
    "np.save(\"avg_word_len\",avg_word_len)\n",
    "\n",
    "\n",
    "count_noun = np.array(count_noun)\n",
    "np.save(\"count_noun\",count_noun)\n",
    "\n",
    "\n",
    "count_adj = np.array(count_adj)\n",
    "np.save(\"count_adj\",count_adj)\n",
    "\n",
    "count_verb = np.array(count_verb)\n",
    "np.save(\"count_verb\",count_verb)\n",
    "\n",
    "count_adv = np.array(count_adv)\n",
    "np.save(\"count_adv\",count_adv)\n",
    "\n",
    "count_lemmas = np.array(count_lemmas)\n",
    "np.save(\"count_lemmas\",count_lemmas)\n",
    "\n",
    "count_spell_error = np.array(count_spell_error)\n",
    "np.save(\"count_spell_error\",count_spell_error)\n",
    "\n",
    "essay_set = np.array(essay_set)\n",
    "essay_set = essay_set[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_counts_X = np.load(\"word_counts_X.npy\")\n",
    "sent_counts = np.load(\"sent_counts.npy\")\n",
    "avg_word_len = np.load(\"avg_word_len.npy\")\n",
    "count_noun = np.load(\"count_noun.npy\")\n",
    "count_adj  = np.load(\"count_adj.npy\")\n",
    "count_verb = np.load(\"count_verb.npy\")\n",
    "count_lemmas = np.load(\"count_lemmas.npy\")\n",
    "count_adv = np.load(\"count_adv.npy\")\n",
    "count_spell_error = np.load(\"count_spell_error.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_X = word_counts_X[indices]\n",
    "sent_counts = sent_counts[indices]\n",
    "avg_word_len  = avg_word_len[indices]\n",
    "count_noun  = count_noun[indices]\n",
    "count_adj  = count_adj[indices]\n",
    "count_verb  = count_verb[indices]\n",
    "count_adv  = count_adv[indices]\n",
    "count_lemmas  = count_lemmas[indices]\n",
    "count_spell_error = count_spell_error[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 3344,\n",
       "         2: 2556,\n",
       "         3: 3699,\n",
       "         4: 3395,\n",
       "         5: 3590,\n",
       "         6: 3594,\n",
       "         7: 3598,\n",
       "         8: 3598,\n",
       "         9: 3596,\n",
       "         10: 3280})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check whether the dataset is balanced or not\n",
    "from collections import Counter\n",
    "Counter(essay_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29112, 500)\n",
      "(29112, 10)\n"
     ]
    }
   ],
   "source": [
    "#Size for train-test split\n",
    "train_size = int(0.85 * len(data))\n",
    "\n",
    "X_train = data[:train_size]\n",
    "X_test = data[train_size:]\n",
    "\n",
    "set_train = essay_set[:train_size]\n",
    "set_test = essay_set[train_size:]\n",
    "\n",
    "avg_word_len_train = avg_word_len[:train_size]\n",
    "avg_word_len_test = avg_word_len[train_size:]\n",
    "\n",
    "count_noun_train = count_noun[:train_size]\n",
    "count_noun_test = count_noun[train_size:]\n",
    "\n",
    "count_adj_train = count_adj[:train_size]\n",
    "count_adj_test = count_adj[train_size:]\n",
    "\n",
    "count_verb_train = count_verb[:train_size]\n",
    "count_verb_test = count_verb[train_size:]\n",
    "\n",
    "count_adv_train = count_adv[:train_size]\n",
    "count_adv_test = count_adv[train_size:]\n",
    "\n",
    "count_spell_error_train = count_spell_error[:train_size]\n",
    "count_spell_error_test = count_spell_error[train_size:]\n",
    "\n",
    "count_lemmas_train = count_lemmas[:train_size]\n",
    "count_lemmas_test = count_lemmas[train_size:]\n",
    "\n",
    "\n",
    "sent_count_train = sent_counts[:train_size]\n",
    "sent_count_test = sent_counts[train_size:]\n",
    "\n",
    "word_count_test = word_counts_X[train_size:]\n",
    "word_count_train = word_counts_X[:train_size]\n",
    "\n",
    "#Concatenate the three data-vectors into a single matrix or feature-set\n",
    "features_train = np.column_stack((set_train,sent_count_train,word_count_train,avg_word_len_train,count_noun_train,count_adj_train,count_verb_train,count_adv_train,count_spell_error_train,count_lemmas_train))\n",
    "features_test = np.column_stack((set_test,sent_count_test,word_count_test,avg_word_len_test,count_noun_test,count_adj_test,count_verb_test,count_adv_test,count_spell_error_test,count_lemmas_test))\n",
    "#features_train = np.column_stack((set_train,sent_count_train,word_count_train))\n",
    "#features_test = np.column_stack((set_test,sent_count_test,word_count_test))\n",
    "\n",
    "y_train = labels[:train_size]\n",
    "y_test = labels[train_size:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(features_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:36: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n",
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, activation=\"softmax\", name=\"score\", kernel_regularizer=<keras.reg..., bias_regularizer=<keras.reg...)`\n",
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 500, 32)      160000      text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 500, 100)     53200       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 150)          150600      lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "features (InputLayer)           (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 160)          0           lstm_18[0][0]                    \n",
      "                                                                 features[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 160)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 200)          32200       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "elu_5 (ELU)                     (None, 200)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "score (Dense)                   (None, 4)            804         elu_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 396,804\n",
      "Trainable params: 396,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Model Design\n",
    "#Functional Model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.layers import LSTM,Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "#Embedding-Output Vector Length\n",
    "embedding_vector_length = 32\n",
    "\n",
    "#Define Text-Input\n",
    "text_in = Input(shape=(500,), name='text')\n",
    "\n",
    "#Embeddings\n",
    "#Used to convert the encoded data i.e the matrix of indices into a form compatible with LSTM\n",
    "embedding = Embedding(output_dim=embedding_vector_length, input_dim=top_words, input_length=500)(text_in)\n",
    "\n",
    "#LSTM\n",
    "lstm_1= LSTM(100, return_sequences = True)(embedding)\n",
    "lstm_2 = LSTM(150)(lstm_1)\n",
    "\n",
    "#Features Inputs (essay_set, word_counts, sent_counts)\n",
    "features_in = Input(shape=(10,), name='features')\n",
    "\n",
    "#Merge layer to merge the output of LSTM and the feature inputs\n",
    "x =  Concatenate()([lstm_2, features_in])\n",
    "\n",
    "#Dropout for the hidden_units to be independent from each other\n",
    "dropout = Dropout(0.2)(x)\n",
    "\n",
    "#Hidden Dense Layer\n",
    "D1 = Dense(200,W_regularizer=l2(0.01),b_regularizer = l2(0.01))(dropout) # try 150 dense after this\n",
    "ED1 = ELU()(D1)\n",
    "\n",
    "# Final Dense Output-Layer\n",
    "score = Dense(4, activation='softmax', name='score',W_regularizer=l2(0.01),b_regularizer = l2(0.01))(ED1) \n",
    "\n",
    "#model\n",
    "model = Model(input=[text_in, features_in], output=[score])\n",
    "\n",
    "#optimizer\n",
    "adam = Adam(lr = 0.001)#, decay = 1e-4) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26200 samples, validate on 2912 samples\n",
      "Epoch 1/1\n",
      " 1152/26200 [>.............................] - ETA: 35:06 - loss: 4.7654 - acc: 0.3429"
     ]
    }
   ],
   "source": [
    "#configure the learning process\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Start Training\n",
    "model.fit([X_train,features_train], y_train, nb_epoch = 1, batch_size=128, validation_split=0.1)  # we pass one data array per model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.61%\n"
     ]
    }
   ],
   "source": [
    "#Save The Trained Model \n",
    "model.save('functional_model.h5')\n",
    "\n",
    "#Predict Score For the test set for calculating accuracy\n",
    "scores = model.evaluate([X_test,features_test], y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
